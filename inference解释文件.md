# ä»£ç åˆ†æè§£é‡Š

*å¯¼å‡ºæ—¶é—´: 2025/11/28 16:04:07*

---

### User

ğŸ“ **inference.py** (Python)

è¯¦ç»†è§£é‡Šåˆ†æè¿™æ®µä»£ç 

---

### ChatGPT

è¿™æ®µè„šæœ¬æ•´ä½“ä¸Šæ˜¯ä¸€ä¸ª GraphCast å¤©æ°”é¢„æŠ¥æ¨¡å‹çš„æ¨ç†+å¯è§†åŒ–è„šæœ¬ï¼š


ä»å‘½ä»¤è¡Œè¯»å–å‚æ•°


åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®


ç”¨ GraphCast åšä¸€æ¬¡æ»šåŠ¨æ¨ç†ï¼ˆrolloutï¼‰


æŠŠçœŸå®å€¼ã€é¢„æµ‹å€¼å’Œå®ƒä»¬çš„å·®ç”»å›¾å¹¶ä¿å­˜ä¸ºå›¾ç‰‡


ä¸‹é¢æˆ‘æŒ‰â€œä»ä¸Šåˆ°ä¸‹ + æ•´ä½“æµç¨‹â€æ¥è¯¦ç»†è®²ä¸€éã€‚inference

1. æ•´ä½“ç»“æ„æ¦‚è§ˆ
æ–‡ä»¶å¤§è‡´åˆ†ä¸ºå‡ å—ï¼š


å‚æ•°å’Œå°å·¥å…·å‡½æ•°ï¼šargsparserã€print_argumentsã€parse_file_parts


æ•°æ®å¤„ç†å’Œç”»å›¾ç›¸å…³å‡½æ•°ï¼šselectã€scaleã€plot_dataã€save_var_diff


æ•°æ®åˆæ³•æ€§æ£€æŸ¥å’Œå½’ä¸€åŒ–æ•°æ®åŠ è½½ï¼šdata_valid_for_modelã€load_data


æ„å»º GraphCast æ¨¡å‹ï¼šconstruct_wrapped_graphcast


Haiku transform çš„å‰å‘/æŸå¤±/æ¢¯åº¦å‡½æ•°ï¼šrun_forwardã€loss_fnã€grads_fn


main() ä¸»æµç¨‹ï¼š


è§£æå‘½ä»¤è¡Œå‚æ•°


åŠ è½½æ¨¡å‹ï¼ˆéšæœº or Checkpointï¼‰


åŠ è½½æ•°æ®å¹¶æ‹†åˆ† inputs/targets/forcings


JIT åŒ…è£…æ¨¡å‹ã€æŸå¤±å’Œæ¢¯åº¦å‡½æ•°


åšæ¨ç† rollout


å¯è§†åŒ–ç»“æœ





2. å‘½ä»¤è¡Œå‚æ•°å’Œå·¥å…·å‡½æ•°
2.1 argsparser()
pythonå¤åˆ¶ä»£ç def argsparser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--pretrained",
        type=str,
        default="./",
        help="pretrained model")
    parser.add_argument(
        "--dataset",
        type=str,
        help="input data")
    parser.add_argument(
        "--mode",
        type=str,
        help="the ways of getting model params")
    parser.add_argument(
        "--var",
        type=str,
        default="2m_temperature",
        help="visualizing atmospheric variables")
    parser.add_argument(
        "--level",
        type=int,
        default=500,
        help="atmospheric pressure level")
    return parser

å‘½ä»¤è¡Œä¼ å‚ï¼š


--pretrained: é¢„è®­ç»ƒæ¨¡å‹ checkpoint æ–‡ä»¶è·¯å¾„


--dataset: è¾“å…¥æ•°æ®ï¼ˆNetCDF æ–‡ä»¶ï¼‰è·¯å¾„


--mode: è·å–æ¨¡å‹å‚æ•°çš„æ–¹å¼ï¼Œä»£ç ä¸­åªç”¨åˆ°äº† "Random" æˆ– "Checkpoint"


--var: æƒ³è¦å¯è§†åŒ–çš„æ°”è±¡å˜é‡åï¼Œé»˜è®¤ "2m_temperature"


--level: å¯è§†åŒ–çš„æ°”å‹å±‚ï¼Œé»˜è®¤ 500 hPa


2.2 print_arguments(args)
ç®€å•æ‰“å°å‚æ•°ï¼Œæ–¹ä¾¿ç¡®è®¤è¿è¡Œæ—¶é…ç½®ã€‚
2.3 parse_file_parts(file_name)
pythonå¤åˆ¶ä»£ç def parse_file_parts(file_name):
    return dict(part.split("-", 1) for part in file_name.split("_"))

å‡è®¾æ–‡ä»¶åå½¢å¦‚ï¼š
source-era5_res-0.25_levels-13_...


æŒ‰ _ åˆ‡åˆ† -> æ¯å—åƒ source-era5


å†æŒ‰ - åˆ‡åˆ†æˆ key/value -> {"source": "era5", "res": "0.25", "levels": "13", ...}


åé¢ç”¨å®ƒæ¥æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ¹é…æ¨¡å‹é…ç½®ã€‚

3. æ•°æ®é€‰æ‹©ä¸ç”»å›¾ç›¸å…³å‡½æ•°
3.1 select(...)ï¼šä» xarray Dataset é‡Œé€‰å˜é‡/å±‚/æ—¶é—´æ­¥
pythonå¤åˆ¶ä»£ç def select(
    data: xarray.Dataset,
    variable: str,
    level: Optional[int] = None,
    max_steps: Optional[int] = None
    ) -> xarray.Dataset:
    data = data[variable]
    if "batch" in data.dims:
        data = data.isel(batch=0)
    if max_steps is not None and "time" in data.sizes and max_steps < data.sizes["time"]:
        data = data.isel(time=range(0, max_steps))
    if level is not None and "level" in data.coords:
        data = data.sel(level=level)
    return data

åšäº†å‡ ä»¶äº‹ï¼š


é€‰å‡ºæŒ‡å®šå˜é‡ variable


å¦‚æœæœ‰ batch ç»´åº¦ï¼Œåªå– batch=0ï¼ˆåªçœ‹ç¬¬ä¸€æ ·æœ¬ï¼‰


å¦‚æœæŒ‡å®š max_stepsï¼Œå¹¶ä¸”æ—¶é—´æ­¥æ•°å¤§äº max_stepsï¼Œåˆ™åªå–å‰ max_steps ä¸ªæ—¶é—´æ­¥


å¦‚æœæŒ‡å®š level ä¸”æœ‰ level åæ ‡ï¼Œåˆ™é€‰å®šæŒ‡å®šæ°”å‹å±‚


3.2 scale(...)ï¼šè®¡ç®—ç”»å›¾æ—¶çš„å½’ä¸€åŒ–èŒƒå›´ä¸ colormap
pythonå¤åˆ¶ä»£ç def scale(
    data: xarray.Dataset,
    center: Optional[float] = None,
    robust: bool = False,
    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:
    vmin = np.nanpercentile(data, (2 if robust else 0))
    vmax = np.nanpercentile(data, (98 if robust else 100))
    if center is not None:
        diff = max(vmax - center, center - vmin)
        vmin = center - diff
        vmax = center + diff
    return (data, matplotlib.colors.Normalize(vmin, vmax),
          ("RdBu_r" if center is not None else "viridis"))



robust=True æ—¶ç”¨ 2â€“98 ç™¾åˆ†ä½ï¼Œé¿å…æç«¯å€¼å½±å“è‰²æ¡èŒƒå›´


å¦‚æœæŒ‡å®šäº† centerï¼ˆä¾‹å¦‚å·®å€¼å›¾ä¸­å¿ƒä¸º 0ï¼‰ï¼Œä¼šè®© vmin/vmax å¯¹ç§°åœ°å›´ç»• center


è¿”å›ï¼š


åŸæ•°æ®


Normalize å¯¹è±¡ï¼ˆæ§åˆ¶é¢œè‰²æ˜ å°„èŒƒå›´ï¼‰


ä½¿ç”¨çš„ colormapï¼š


æœ‰ center ç”¨ RdBu_rï¼ˆçº¢è“åè½¬ï¼Œé€‚åˆåå·®ï¼‰


å¦åˆ™ç”¨ viridis






3.3 plot_data(...)ï¼šç”»å¤šä¸ªå­å›¾ï¼ˆç›®æ ‡/é¢„æµ‹/å·®å€¼ï¼‰
pythonå¤åˆ¶ä»£ç def plot_data(
    data: dict[str, xarray.Dataset],
    fig_title: str,
    plot_size: float = 5,
    robust: bool = False,
    cols: int = 4,
    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:

è¿™é‡Œ data ç»“æ„ç±»ä¼¼ï¼š
pythonå¤åˆ¶ä»£ç {
  "Targets": (data_xarray, norm, cmap),
  "Predictions": (...),
  "Diff": (...)
}

ä¸»è¦æ­¥éª¤ï¼š


ä»ç¬¬ä¸€ä¸ªæ•°æ®ä¸­å–å‡ºæ—¶é—´ç»´å¤§å°ï¼Œç”¨æ¥ç¡®è®¤æ‰€æœ‰æ•°æ®æ—¶é—´æ­¥æ•°ä¸€è‡´


æ ¹æ® len(data) å’Œ cols è®¡ç®—è¡Œåˆ—æ•°ï¼Œåˆ›å»º figure


å¯¹æ¯ä¸ª (title, (plot_data, norm, cmap))ï¼š


åˆ›å»ºå­å›¾


éšè—åæ ‡è½´


å›¾åç”¨ titleï¼ˆå¦‚ "Targets", "Predictions", "Diff"ï¼‰


ç”¨ imshow æŠŠç¬¬ 0 ä¸ªæ—¶é—´æ­¥ç”»å‡ºæ¥


åŠ  colorbar




æ¯æ¬¡å¾ªç¯é‡Œè°ƒç”¨ plt.savefig(f"{fig_title}.png")ï¼ˆè¿™é‡Œæœ‰ç‚¹å°é—®é¢˜ï¼šåœ¨å¾ªç¯é‡Œä¿å­˜ä¼šé‡å¤è¦†ç›–åŒä¸€ä¸ªæ–‡ä»¶ï¼Œå¤šä½™ï¼Œä½†ä¸å½±å“æ ¸å¿ƒé€»è¾‘ï¼‰


3.4 save_var_diff(...)ï¼šä¸“é—¨ç”»ç›®æ ‡ vs é¢„æµ‹ vs å·®å€¼
pythonå¤åˆ¶ä»£ç def save_var_diff(eval_targets, predictions, plot_pred_variable, plot_pred_level, plot_max_steps=1):
    plot_size = 5
    plot_max_steps = min(predictions.sizes["time"], 1)

    data = {
        "Targets": scale(select(eval_targets, plot_pred_variable, plot_pred_level, plot_max_steps), robust=True),
        "Predictions": scale(select(predictions, plot_pred_variable, plot_pred_level, plot_max_steps), robust=True),
        "Diff": scale((select(eval_targets, plot_pred_variable, plot_pred_level, plot_max_steps) -
                            select(predictions, plot_pred_variable, plot_pred_level, plot_max_steps)),
                          robust=True, center=0),
    }
    fig_title = plot_pred_variable
    if "level" in predictions[plot_pred_variable].coords:
      fig_title += f"_at_{plot_pred_level}_hPa"

    plot_data(data, fig_title, plot_size, True)



ä½¿ç”¨ä¸Šé¢çš„ select å’Œ scaleï¼š


Targets: çœŸå®å€¼


Predictions: é¢„æµ‹å€¼


Diff: çœŸå®å€¼ - é¢„æµ‹å€¼ï¼Œcenter=0 ç”¨çº¢è“è‰²å·®åˆ†è‰²æ¡




å›¾ç‰‡æ–‡ä»¶åç±»ä¼¼ 2m_temperature_at_500_hPa.png



4. å½’ä¸€åŒ–å’Œæ¨¡å‹é…ç½®ç›¸å…³
4.1 data_valid_for_model(...)ï¼ˆç›®å‰æ²¡åœ¨ main ä¸­ç”¨ï¼‰
æ£€æŸ¥æ•°æ®æ–‡ä»¶åæå–å‡ºçš„ä¿¡æ¯æ˜¯å¦ä¸æ¨¡å‹é…ç½®ä¸€è‡´ï¼Œæ¯”å¦‚ï¼š


åˆ†è¾¨ç‡ res


å‹åŠ›å±‚æ•° levels


æºæ•°æ® source ä¸æ˜¯å¦ä½¿ç”¨ total_precipitation_6hr å¯¹åº”


å¦‚æœä¸¥æ ¼éƒ¨ç½²ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥åœ¨åŠ è½½æ•°æ®å‰åš sanity checkã€‚
4.2 load_data()ï¼šåŠ è½½æ ‡å‡†åŒ–ç»Ÿè®¡é‡
pythonå¤åˆ¶ä»£ç def load_data():
    # Load normalization data
    with open("./stats/stats_diffs_stddev_by_level.nc", "rb") as f:
        diffs_stddev_by_level = xarray.load_dataset(f).compute()
    with open("./stats/stats_mean_by_level.nc", "rb") as f:
        mean_by_level = xarray.load_dataset(f).compute()
    with open("./stats/stats_stddev_by_level.nc", "rb") as f:
        stddev_by_level = xarray.load_dataset(f).compute()
    return diffs_stddev_by_level, mean_by_level, stddev_by_level 

ä» ./stats/ ç›®å½•è¯»ä¸‰ä¸ª NetCDF æ–‡ä»¶ï¼ŒåŒ…å«ï¼š


æ¯å±‚çš„å·®åˆ†æ ‡å‡†å·®


æ¯å±‚çš„å‡å€¼


æ¯å±‚çš„æ ‡å‡†å·®


ç”¨äºè¾“å…¥è¾“å‡ºçš„å½’ä¸€åŒ–å’Œæ®‹å·®å½’ä¸€åŒ–ã€‚

5. æ„å»ºå°è£…å¥½çš„ GraphCast æ¨¡å‹
5.1 construct_wrapped_graphcast(...)
pythonå¤åˆ¶ä»£ç def construct_wrapped_graphcast(
    model_config: graphcast.ModelConfig,
    task_config: graphcast.TaskConfig):

    """Constructs and wraps the GraphCast Predictor."""
    # Deeper one-step predictor.
    predictor = graphcast.GraphCast(model_config, task_config)

    # BFloat16 cast
    predictor = casting.Bfloat16Cast(predictor)

    # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
    diffs_stddev_by_level, mean_by_level, stddev_by_level = load_data()
    predictor = normalization.InputsAndResiduals(
        predictor,
        diffs_stddev_by_level=diffs_stddev_by_level,
        mean_by_level=mean_by_level,
        stddev_by_level=stddev_by_level)

    # Wraps everything so the one-step model can produce trajectories.
    predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)
    return predictor

å°è£…é¡ºåºï¼š


GraphCast åŸå§‹ä¸€æ—¶åˆ»é¢„æµ‹å™¨ï¼ˆsingle-stepï¼‰


ç”¨ Bfloat16Cast æŠŠå†…éƒ¨è®¡ç®—è½¬æˆ bfloat16ï¼ˆæ›´çœæ˜¾å­˜ï¼‰


å¤–é¢å†åŒ…ä¸€å±‚ InputsAndResidualsï¼Œè´Ÿè´£ï¼š


å¯¹è¾“å…¥/è¾“å‡ºåšæ ‡å‡†åŒ–ï¼ˆç”¨ä¹‹å‰ load_data() å¾—åˆ°çš„ç»Ÿè®¡é‡ï¼‰


é¢„æµ‹æ®‹å·®ç­‰




æœ€å¤–å±‚åŒ…æˆ autoregressive.Predictorï¼š


å¯ä»¥æ ¹æ®å•æ­¥æ¨¡å‹åå¤æ»šåŠ¨ï¼Œå¾—åˆ°å¤šæ­¥é¢„æµ‹åºåˆ—


æ‰“å¼€ gradient_checkpointing å‡å°‘æ˜¾å­˜ï¼ˆå¦‚æœè®­ç»ƒæ—¶ç”¨ï¼‰





6. Haiku æ¨¡å‹ transform ä¸æŸå¤±/æ¢¯åº¦å‡½æ•°
6.1 run_forwardï¼šå‰å‘æ¨ç†å‡½æ•°
pythonå¤åˆ¶ä»£ç @hk.transform_with_state
def run_forward(model_config, task_config, inputs, targets_template, forcings):
    predictor = construct_wrapped_graphcast(model_config, task_config)
    return predictor(inputs, targets_template=targets_template, forcings=forcings)



@hk.transform_with_state ä¼šæŠŠè¿™ä¸ªå‡½æ•°è½¬æˆ Haiku çš„å½¢å¼ï¼š


init(...) è´Ÿè´£åˆå§‹åŒ–å‚æ•°


apply(params, state, rng, ...) è´Ÿè´£å‰å‘è®¡ç®—




è¾“å…¥ï¼š


inputs: å†å²è¾“å…¥å˜é‡ï¼ˆxarray -> jax arrayï¼‰


targets_template: åªç”¨æ¥æä¾›ç›®æ ‡çš„ shapeï¼ˆè¿™é‡Œä¼ å…¥çš„æ˜¯ eval_targets * np.nanï¼‰


forcings: å¤–éƒ¨å¼ºè¿«ï¼ˆå¦‚å¤ªé˜³è¾å°„ç­‰ï¼‰




6.2 loss_fnï¼šå¸¦è¯Šæ–­çš„æŸå¤±å‡½æ•°
pythonå¤åˆ¶ä»£ç @hk.transform_with_state
def loss_fn(model_config, task_config, inputs, targets, forcings):

    predictor = construct_wrapped_graphcast(model_config, task_config)
    loss, diagnostics = predictor.loss(inputs, targets, forcings)
    return xarray_tree.map_structure(
        lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),
        (loss, diagnostics))



è°ƒç”¨ predictor.loss(...) è®¡ç®—ï¼š


æ ‡é‡æŸå¤±ï¼ˆå¸¦ time/space/variable ç»´åº¦ï¼‰


å…¶ä»–è¯Šæ–­æŒ‡æ ‡




å†ç”¨ xarray_tree.map_structure æŠŠæ¯ä¸ª xarray é‡Œçš„æ•°æ®å– meanï¼ˆå…¨å±€å¹³å‡ï¼‰ï¼Œå¹¶ unwrap æˆçº¯ JAX æ•°ç»„ã€‚


6.3 grads_fnï¼šè®¡ç®—æŸå¤±åŠå…¶æ¢¯åº¦
pythonå¤åˆ¶ä»£ç def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):

    def _aux(params, state, i, t, f):
        (loss, diagnostics), next_state = loss_fn.apply(
            params, state, jax.random.PRNGKey(0), model_config, task_config,
            i, t, f)
        return loss, (diagnostics, next_state)
    (loss, (diagnostics, next_state)), grads = jax.value_and_grad(
        _aux, has_aux=True)(params, state, inputs, targets, forcings)
    return loss, diagnostics, next_state, grads



_aux å°è£…äº†è°ƒç”¨ loss_fn.apply çš„é€»è¾‘ï¼Œè¿”å› (loss, (diagnostics, next_state))


ç”¨ jax.value_and_grad å¯¹ params æ±‚æ¢¯åº¦


è¿”å› lossã€diagnosticsã€next_state å’Œ grads
ï¼ˆåœ¨ main é‡Œè¿™å—è®­ç»ƒä»£ç è¢«æ³¨é‡Šæ‰äº†ï¼‰



7. main() ä¸»æµç¨‹
7.1 è§£æå‚æ•°å¹¶æ‰“å°
pythonå¤åˆ¶ä»£ç parser = argsparser()
FLAGS = parser.parse_args()
print_arguments(FLAGS)

7.2 åŠ è½½æˆ–æ„å»ºæ¨¡å‹
pythonå¤åˆ¶ä»£ç source = FLAGS.mode
if source == "Random":
    params = None  # Filled in below
    state = {}
    model_config = graphcast.ModelConfig(...)
    task_config = graphcast.TaskConfig(...)
else:
    assert source == "Checkpoint"
    with open(FLAGS.pretrained, "rb") as f:
        ckpt = checkpoint.load(f, graphcast.CheckPoint)
    params = ckpt.params
    state = {}
    model_config = ckpt.model_config
    task_config = ckpt.task_config

ä¸¤ç§æ¨¡å¼ï¼š


Random:


ä¸åŠ è½½ checkpointï¼Œparams=Noneï¼Œéšåä¼šç”¨ run_forward.init éšæœºåˆå§‹åŒ–


ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹é…ç½®ï¼ˆlatent_size=32 ç­‰ï¼‰




Checkpointï¼š


ä» FLAGS.pretrained æŒ‡å®šçš„æ–‡ä»¶åŠ è½½ CheckPoint


é‡Œé¢åŒ…å« paramsã€model_configã€task_configã€æè¿°å’Œ license


æ‰“å°æ¨¡å‹ä¿¡æ¯




7.3 åŠ è½½æ•°æ®å¹¶æ‹†åˆ† inputs/targets/forcings
pythonå¤åˆ¶ä»£ç dataset_file = FLAGS.dataset
with open(dataset_file, "rb") as f:
    example_batch = xarray.load_dataset(f).compute()
assert example_batch.sizes["time"] >= 3
print(", ".join([...parse_file_parts...]))

train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(
    example_batch, target_lead_times=slice("6h", f"{1 * 6}h"),
    **dataclasses.asdict(task_config))

eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(
    example_batch, target_lead_times=slice("6h", f"{1 * 6}h"),
    **dataclasses.asdict(task_config))



xarray.load_dataset è¯»å– NetCDF æ•°æ®é›†


éœ€è¦è‡³å°‘ 3 ä¸ª time stepï¼ˆ2 ä¸ªè¾“å…¥ + >=1 ä¸ªç›®æ ‡ï¼‰


extract_inputs_targets_forcings ä¼šï¼š


æ ¹æ® task_config ä¸­çš„å˜é‡åˆ—è¡¨ã€æ—¶é—´é•¿åº¦ç­‰ï¼Œä» example_batch ä¸­æ‹†åˆ†å‡ºï¼š


inputsï¼ˆæ¯”å¦‚è¿‡å» 24 å°æ—¶ï¼‰


targetsï¼ˆæœªæ¥ 6 å°æ—¶ï¼‰


forcingsï¼ˆå¤–éƒ¨å¼ºè¿«ï¼‰






è¿™é‡Œ train å’Œ eval ç”¨çš„æ˜¯åŒä¸€æ‰¹æ•°æ®ï¼Œåªæ˜¯åˆ†åˆ«å‘½åã€‚
7.4 è¾…åŠ©åŒ…è£…å‡½æ•°ï¼šwith_configs / with_params / drop_state
pythonå¤åˆ¶ä»£ç def with_configs(fn):
    return functools.partial(
        fn, model_config=model_config, task_config=task_config)

def with_params(fn):
    return functools.partial(fn, params=params, state=state)

def drop_state(fn):
    return lambda **kw: fn(**kw)[0]

ç”¨é€”ï¼š


JAX çš„ jit ä¸€èˆ¬ä¸å–œæ¬¢å¯¹å¤æ‚å¯¹è±¡ï¼ˆå¦‚ config dataclassï¼‰åšé—­åŒ…æ•è·ï¼Œå› æ­¤ç”¨ partial æ–¹å¼ä½œä¸ºæ˜¾å¼å‚æ•°ã€‚


drop_stateï¼šæœ‰äº›å‡½æ•°è¿”å› (output, state)ï¼Œä½†è¿™é‡Œæ¨¡å‹å…¶å®æ— çŠ¶æ€ï¼Œæ‰€ä»¥åªå– [0]ã€‚


7.5 åˆå§‹åŒ–å‚æ•°ï¼ˆå¦‚æœæ˜¯ Random æ¨¡å¼ï¼‰
pythonå¤åˆ¶ä»£ç init_jitted = jax.jit(with_configs(run_forward.init))

if params is None:
    params, state = init_jitted(
        rng=jax.random.PRNGKey(0),
        inputs=train_inputs,
        targets_template=train_targets,
        forcings=train_forcings)



ç”¨ run_forward.init æ ¹æ®è¾“å…¥æ•°æ® shape åˆå§‹åŒ– Haiku å‚æ•°


åˆå§‹åŒ–æ—¶ä¹Ÿè¦ä¼ å…¥ model_config, task_config ä½œä¸ºé™æ€å‚æ•°


7.6 JIT åŒ–æŸå¤±ã€æ¢¯åº¦ã€å‰å‘æ¨ç†å‡½æ•°
pythonå¤åˆ¶ä»£ç loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))
grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))
run_forward_jitted = drop_state(with_params(with_configs(
    run_forward.apply)))



loss_fn_jitted(rng=..., inputs=..., targets=..., forcings=...) è¿”å›æŸå¤±å’Œè¯Šæ–­


grads_fn_jitted(inputs=..., targets=..., forcings=...) è®¡ç®—æ¢¯åº¦


run_forward_jitted(rng=..., inputs=..., targets_template=..., forcings=...) åšå‰å‘é¢„æµ‹


7.7 åˆ†è¾¨ç‡æ£€æŸ¥
pythonå¤åˆ¶ä»£ç assert model_config.resolution in (0, 360. / eval_inputs.sizes["lon"]), (
    "Model resolution doesn't match the data resolution. ..."
)

ç¡®ä¿æ¨¡å‹å’Œæ•°æ®çš„ç»çº¬åº¦åˆ†è¾¨ç‡ä¸€è‡´ï¼Œå¦åˆ™è¯´æ˜æ•°æ®å’Œæ¨¡å‹ä¸åŒ¹é…ã€‚
7.8 rollout æ¨ç†
pythonå¤åˆ¶ä»£ç predictions = rollout.chunked_prediction(
    run_forward_jitted,
    rng=jax.random.PRNGKey(0),
    inputs=eval_inputs,
    targets_template=eval_targets * np.nan,
    forcings=eval_forcings)



rollout.chunked_prediction ä¼šå¤šæ­¥æ»šåŠ¨é¢„æµ‹ï¼š


åˆå§‹ç”¨ inputs é¢„æµ‹ targets_template å½¢çŠ¶çš„æ•°æ®


ç„¶åæŠŠé¢„æµ‹ç»“æœä½œä¸ºæ–°çš„è¾“å…¥å†å¾€å‰æ¨ï¼ˆå¦‚æœéœ€è¦å¤šæ—¶é—´æ­¥ï¼‰




è¿™é‡Œ targets_template = eval_targets * np.nan çš„ç›®çš„æ˜¯ï¼š


ç”¨çœŸå®ç›®æ ‡çš„ shape å¡«å…… NaNï¼Œç”¨äºå‘Šè¯‰æ¨¡å‹æœŸæœ›è¾“å‡ºçš„å¤§å°




7.9 é¢„æµ‹ç»“æœå¯è§†åŒ–
pythonå¤åˆ¶ä»£ç save_var_diff(eval_targets, predictions, FLAGS.var, FLAGS.level)
print("----------------------------graphcast inference results----------------------------")
print(predictions)



ç”»å›¾ï¼ˆçœŸå® vs é¢„æµ‹ vs å·®å€¼ï¼‰ï¼Œå›¾ç‰‡ä¿å­˜åˆ°å½“å‰ç›®å½•


æ§åˆ¶å˜é‡å FLAGS.var å’Œå±‚æ¬¡ FLAGS.level


æ‰“å°æ•´ä¸ª predictions çš„ xarray ç»“æ„


7.10 è®­ç»ƒç›¸å…³ä»£ç ï¼ˆå·²æ³¨é‡Šï¼‰
ä¸‹é¢ä¸€å¤§æ®µå…³äºï¼š


è®¡ç®—è®­ç»ƒæŸå¤±


åå‘ä¼ æ’­æ±‚æ¢¯åº¦


å†ç”¨ run_forward_jitted é¢„æµ‹


éƒ½è¢«æ³¨é‡Šæ‰äº†ï¼Œä¿ç•™ä½œå‚è€ƒã€‚

8. å¦‚ä½•ä½¿ç”¨è¿™æ®µè„šæœ¬ï¼ˆç¤ºä¾‹ï¼‰
å‡è®¾ä½ å·²ç»æœ‰ï¼š


é¢„è®­ç»ƒæ¨¡å‹ checkpointï¼š./graphcast.ckpt


æ•°æ®ï¼š./data/sample.nc


å‘½ä»¤å¯èƒ½åƒè¿™æ ·ï¼š
bashå¤åˆ¶ä»£ç python inference.py \
  --mode Checkpoint \
  --pretrained ./graphcast.ckpt \
  --dataset ./data/sample.nc \
  --var 2m_temperature \
  --level 500

è¿è¡Œåï¼Œä½ ä¼šå¾—åˆ°ç±»ä¼¼ï¼š


2m_temperature_at_500_hPa.pngï¼šçœŸå®/é¢„æµ‹/å·®å€¼å›¾


æ§åˆ¶å°æ‰“å°æ¨¡å‹ä¿¡æ¯ã€è¾“å…¥/ç›®æ ‡ç»´åº¦ç»“æ„ï¼Œä»¥åŠ predictions çš„ xarray æ•°æ®



å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ é€è¡ŒåŠ ä¸­æ–‡æ³¨é‡Šç‰ˆæœ¬ï¼Œæˆ–è€…å¸®ä½ æ”¹é€ æˆåªåšå¤šæ­¥é¢„æµ‹ï¼ˆæ¯”å¦‚è¿æ»š 4 ä¸ª 6h é¢„æµ‹ï¼Œæ€»å…± 24hï¼‰å¹¶ä¿å­˜æ‰€æœ‰æ—¶é—´æ­¥çš„å¯è§†åŒ–ã€‚

